{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Question 1: What is Deep Learning? Briefly describe how it evolved and how it differs from traditional machine learning.\n",
    "    ->Deep Learning is a subset of Machine Learning that uses artificial neural networks \n",
    "   with multiple layers to automatically learn and extract complex patterns from large \n",
    "   amounts of data.\n",
    "\n",
    "        Evolution of Deep Learning:\n",
    "\n",
    "        1. 1950s–1980s:\n",
    "           - Introduction of the Perceptron (1958) as the first neural network model.\n",
    "           - Development of the backpropagation algorithm (1986) enabled training of \n",
    "             multi-layer networks.\n",
    "           - Limited progress due to lack of computational power and small datasets.\n",
    "\n",
    "        2. 1990s–2000s:\n",
    "           - Traditional ML algorithms (like SVM, Decision Trees) dominated because \n",
    "             neural networks were hard to train.\n",
    "           - Research in neural networks continued but with limited practical success.\n",
    "\n",
    "        3. 2010s–Present:\n",
    "           - Rapid growth due to big data, GPUs, and improved algorithms.\n",
    "           - AlexNet (2012) marked a breakthrough in image recognition.\n",
    "           - Deep Learning became the backbone of modern AI applications such as \n",
    "             speech recognition, NLP, and self-driving cars.\n",
    "\n",
    "        Difference Between Deep Learning and Traditional Machine Learning:\n",
    "\n",
    "        1. Feature Extraction:\n",
    "           - ML: Requires manual feature engineering.\n",
    "           - DL: Learns features automatically from raw data.\n",
    "\n",
    "        2. Data Requirement:\n",
    "           - ML: Works well on small or medium datasets.\n",
    "           - DL: Needs large amounts of labeled data.\n",
    "\n",
    "        3. Computation Power:\n",
    "           - ML: Less computationally intensive.\n",
    "           - DL: Requires high computational resources (GPUs/TPUs).\n",
    "\n",
    "        4. Model Complexity:\n",
    "           - ML: Uses simple models like Decision Trees or SVM.\n",
    "           - DL: Uses deep neural networks with many layers.\n",
    "\n",
    "        5. Interpretability:\n",
    "           - ML: Models are easier to interpret and explain.\n",
    "           - DL: Often behaves like a “black box.”\n",
    "\n",
    "        6. Applications:\n",
    "           - ML: Fraud detection, spam filtering, customer segmentation.\n",
    "           - DL: Image recognition, speech processing, natural language understanding.\n",
    "\n",
    "        In short:\n",
    "        Deep Learning evolved from basic neural networks into a powerful approach \n",
    "        that outperforms traditional ML in handling complex, high-dimensional data.\n",
    "\n",
    "Question 2: Explain the basic architecture and functioning of a Perceptron. What are its\n",
    "limitations?\n",
    "        ->A Perceptron is the simplest type of artificial neural network and forms the \n",
    "       basic building block of Deep Learning models. It is a single-layer neural \n",
    "       network used mainly for binary classification tasks.\n",
    "\n",
    "        2. Basic Architecture:\n",
    "           - Input Layer: Accepts input features (x1, x2, x3, ... , xn).\n",
    "           - Weights (w1, w2, w3, ... , wn): Each input is assigned a weight that \n",
    "             determines its importance.\n",
    "           - Summation Function: Computes the weighted sum of all inputs.\n",
    "               → Net Input = (w1*x1 + w2*x2 + ... + wn*xn) + bias\n",
    "           - Activation Function: Applies a step function to the net input to produce \n",
    "             an output (usually 0 or 1).\n",
    "\n",
    "        3. Working of a Perceptron:\n",
    "           - Step 1: Inputs are multiplied by their corresponding weights.\n",
    "           - Step 2: Weighted inputs are summed and bias is added.\n",
    "           - Step 3: The result is passed through an activation function (e.g., step function).\n",
    "           - Step 4: The perceptron outputs a value (0 or 1) indicating the predicted class.\n",
    "           - Step 5: During training, weights are updated based on the error using the \n",
    "             Perceptron Learning Rule.\n",
    "\n",
    "             Learning Rule: \n",
    "             w_new = w_old + η * (y_actual - y_predicted) * x\n",
    "             where η = learning rate\n",
    "\n",
    "        4. Example:\n",
    "           - Inputs: x1, x2\n",
    "           - Weights: w1, w2\n",
    "           - Bias: b\n",
    "           - Output: y = step(w1*x1 + w2*x2 + b)\n",
    "\n",
    "        5. Limitations of Perceptron:\n",
    "           - Can only solve linearly separable problems (e.g., AND, OR gates).\n",
    "           - Fails on non-linear problems like XOR.\n",
    "           - Limited learning capability — cannot handle complex patterns or data.\n",
    "           - Uses a simple step activation function, making it non-differentiable and \n",
    "             unsuitable for gradient-based optimization.\n",
    "           - No hidden layers — hence cannot perform feature extraction or hierarchical learning.\n",
    "\n",
    "\n",
    "Question 3: Describe the purpose of activation function in neural networks. Compare\n",
    "Sigmoid, ReLU, and Tanh functions.\n",
    "    -> 1. Purpose of Activation Function:\n",
    "       - The activation function determines whether a neuron should be activated or not.\n",
    "       - It introduces non-linearity into the network, allowing it to learn complex and \n",
    "         non-linear relationships between input and output.\n",
    "       - Without activation functions, a neural network would behave like a simple \n",
    "         linear model, regardless of how many layers it has.\n",
    "       - Activation functions help neural networks generalize better and perform tasks \n",
    "         such as image recognition, language translation, and pattern detection.\n",
    "\n",
    "    2. Common Activation Functions:\n",
    "\n",
    "       a) Sigmoid Function:\n",
    "          - Formula: f(x) = 1 / (1 + e^(-x))\n",
    "          - Range: (0, 1)\n",
    "          - Nature: Smooth, S-shaped curve.\n",
    "          - Advantages:\n",
    "            • Converts input into a probability-like output between 0 and 1.\n",
    "            • Useful for binary classification.\n",
    "          - Disadvantages:\n",
    "            • Causes vanishing gradient problem for large positive or negative inputs.\n",
    "            • Output not zero-centered, which slows down convergence.\n",
    "\n",
    "       b) Tanh (Hyperbolic Tangent) Function:\n",
    "          - Formula: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "          - Range: (-1, 1)\n",
    "          - Nature: S-shaped curve like Sigmoid but zero-centered.\n",
    "          - Advantages:\n",
    "            • Produces stronger gradients than Sigmoid.\n",
    "            • Output is zero-centered, leading to faster training.\n",
    "          - Disadvantages:\n",
    "            • Still suffers from the vanishing gradient problem for large inputs.\n",
    "\n",
    "       c) ReLU (Rectified Linear Unit) Function:\n",
    "          - Formula: f(x) = max(0, x)\n",
    "          - Range: [0, ∞)\n",
    "          - Nature: Linear for positive inputs and zero for negative inputs.\n",
    "          - Advantages:\n",
    "            • Computationally efficient and easy to implement.\n",
    "            • Helps reduce vanishing gradient problem.\n",
    "            • Speeds up convergence.\n",
    "          - Disadvantages:\n",
    "            • Can cause \"dying ReLU\" problem when neurons output zero for all inputs.\n",
    "            • Not zero-centered.\n",
    "\n",
    "    3. Comparison Table:\n",
    "\n",
    "       Activation Function | Range      | Non-Linearity | Zero-Centered | Issues\n",
    "       -------------------- | ---------- | -------------- | -------------- | --------------------------\n",
    "       Sigmoid              | (0, 1)     | Yes            | No             | Vanishing gradients\n",
    "       Tanh                 | (-1, 1)    | Yes            | Yes            | Vanishing gradients (less)\n",
    "       ReLU                 | [0, ∞)     | Yes            | No             | Dying ReLU problem\n",
    "\n",
    "\n",
    "Question 4: What is the difference between Loss function and Cost function in neural\n",
    "networks? Provide examples.\n",
    "    ->1. Definition of Loss Function:\n",
    "       - A Loss Function measures the error for a single training example.\n",
    "       - It calculates how far the predicted output is from the actual output.\n",
    "       - The goal is to minimize this loss during training so that the model makes \n",
    "         more accurate predictions.\n",
    "\n",
    "       Example:\n",
    "       - Mean Squared Error (MSE) for one sample:\n",
    "           Loss = (y_actual - y_predicted)²\n",
    "       - Binary Cross-Entropy Loss:\n",
    "           Loss = -[y*log(p) + (1 - y)*log(1 - p)]\n",
    "\n",
    "    2. Definition of Cost Function:\n",
    "       - A Cost Function is the average (or total) of all individual losses across \n",
    "         the entire training dataset.\n",
    "       - It represents the model’s overall performance.\n",
    "       - The neural network optimizes the cost function using techniques like \n",
    "         Gradient Descent.\n",
    "\n",
    "       Example:\n",
    "       - For n samples:\n",
    "           Cost = (1/n) * Σ (Loss for each training example)\n",
    "       - In the case of MSE:\n",
    "           Cost = (1/n) * Σ (y_actual - y_predicted)²\n",
    "\n",
    "    3. Key Differences Between Loss Function and Cost Function:\n",
    "\n",
    "       Aspect                | Loss Function                         | Cost Function\n",
    "       ---------------------- | -------------------------------------- | --------------------------------------\n",
    "       Definition             | Error for a single training example    | Average of all losses in the dataset\n",
    "       Scope                  | Individual sample                     | Entire dataset\n",
    "       Purpose                | Measures individual prediction error   | Measures overall model performance\n",
    "       Optimization Target    | Not directly optimized                 | Directly minimized by the model\n",
    "       Example                | Cross-Entropy Loss for one data point  | Mean Cross-Entropy for all data points\n",
    "\n",
    "    4. Examples:\n",
    "       - Loss Function: Cross-Entropy Loss for a single image classification sample.\n",
    "       - Cost Function: Average Cross-Entropy Loss over all images in the training set.\n",
    "       \n",
    "       \n",
    "Question 5: What is the role of optimizers in neural networks? Compare Gradient\n",
    "Descent, Adam, and RMSprop.\n",
    "    ->1. Role of Optimizers:\n",
    "       - Optimizers are algorithms used to adjust the weights and biases of a neural network \n",
    "         to minimize the cost (loss) function.\n",
    "       - They control how the model learns from the data by updating parameters based on \n",
    "         the gradients (direction of steepest descent).\n",
    "       - The main goal of an optimizer is to find the global minimum of the loss function \n",
    "         as efficiently and accurately as possible.\n",
    "\n",
    "           General weight update rule:\n",
    "             w_new = w_old - η * (∂Loss/∂w)\n",
    "             where η = learning rate\n",
    "\n",
    "    2. Types of Optimizers:\n",
    "\n",
    "       a) Gradient Descent:\n",
    "          - Basic optimization algorithm that updates weights in the direction \n",
    "            opposite to the gradient of the loss function.\n",
    "          - Variants:\n",
    "            • Batch Gradient Descent: Uses the entire dataset for one update.\n",
    "            • Stochastic Gradient Descent (SGD): Updates weights after each sample.\n",
    "            • Mini-Batch Gradient Descent: Updates weights after a small batch of samples.\n",
    "          - Advantages:\n",
    "            • Simple and easy to implement.\n",
    "          - Disadvantages:\n",
    "            • Can get stuck in local minima.\n",
    "            • Slow convergence on large datasets.\n",
    "            • Sensitive to the choice of learning rate.\n",
    "\n",
    "       b) RMSprop (Root Mean Square Propagation):\n",
    "          - Modifies SGD by maintaining a moving average of squared gradients \n",
    "            for each parameter.\n",
    "          - It adjusts the learning rate for each weight individually.\n",
    "          - Formula:\n",
    "            v = β * v + (1 - β) * (∂Loss/∂w)²\n",
    "            w_new = w_old - (η / √(v + ε)) * (∂Loss/∂w)\n",
    "          - Advantages:\n",
    "            • Adapts learning rate automatically.\n",
    "            • Works well for non-stationary problems (e.g., RNNs).\n",
    "          - Disadvantages:\n",
    "            • May still be unstable for very deep networks.\n",
    "\n",
    "       c) Adam (Adaptive Moment Estimation):\n",
    "          - Combines the advantages of both RMSprop and Momentum optimization.\n",
    "          - Maintains moving averages of both gradients (first moment) \n",
    "            and squared gradients (second moment).\n",
    "          - Formula:\n",
    "            m = β1 * m + (1 - β1) * (∂Loss/∂w)\n",
    "            v = β2 * v + (1 - β2) * (∂Loss/∂w)²\n",
    "            w_new = w_old - η * (m / (√v + ε))\n",
    "          - Advantages:\n",
    "            • Fast convergence.\n",
    "            • Works well with large datasets and complex models.\n",
    "            • Less sensitive to learning rate selection.\n",
    "          - Disadvantages:\n",
    "            • Can sometimes overfit or overshoot the minimum.\n",
    "\n",
    "    3. Comparison Table:\n",
    "\n",
    "       Optimizer      | Learning Rate Adaptation | Speed of Convergence | Memory Usage | Common Use Cases\n",
    "       --------------- | ------------------------ | -------------------- | ------------- | ----------------------------\n",
    "       Gradient Descent | Fixed (unless manually tuned) | Slow               | Low          | Small datasets, simple models\n",
    "       RMSprop          | Adaptive per parameter       | Fast               | Moderate     | Recurrent Neural Networks\n",
    "       Adam             | Adaptive with momentum       | Very fast          | Higher       | Deep Neural Networks, NLP, CNNs\n",
    "\n",
    "       \n",
    "       \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6cdcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [0.2 0.1]\n",
      "Trained bias: -0.20000000000000004\n",
      "\n",
      "AND Gate Output:\n",
      "Input: [0 0] -> Output: 0\n",
      "Input: [0 1] -> Output: 0\n",
      "Input: [1 0] -> Output: 0\n",
      "Input: [1 1] -> Output: 1\n"
     ]
    }
   ],
   "source": [
    "#Question 6: Write a Python program to implement a single-layer perceptron from scratch using NumPy to solve the logical AND gate.\n",
    "import numpy as np\n",
    "\n",
    "# Input and output for AND gate\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Initialize weights, bias, and learning parameters\n",
    "weights = np.zeros(X.shape[1])\n",
    "bias = 0\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "\n",
    "# Step activation function\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "# Training the perceptron\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X)):\n",
    "        net_input = np.dot(X[i], weights) + bias\n",
    "        y_pred = step_function(net_input)\n",
    "        error = y[i] - y_pred\n",
    "        weights += learning_rate * error * X[i]\n",
    "        bias += learning_rate * error\n",
    "\n",
    "# Testing the perceptron\n",
    "print(\"Trained weights:\", weights)\n",
    "print(\"Trained bias:\", bias)\n",
    "print(\"\\nAND Gate Output:\")\n",
    "\n",
    "for i in range(len(X)):\n",
    "    net_input = np.dot(X[i], weights) + bias\n",
    "    output = step_function(net_input)\n",
    "    print(f\"Input: {X[i]} -> Output: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffc9f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Question 7: Implement and visualize Sigmoid, ReLU, and Tanh activation functions using Matplotlib.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Generate input values\n",
    "x = np.linspace(-10, 10, 200)\n",
    "\n",
    "# Compute outputs\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_relu = relu(x)\n",
    "y_tanh = tanh(x)\n",
    "\n",
    "# Plot activation functions\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(x, y_sigmoid, color='blue')\n",
    "plt.title(\"Sigmoid Activation Function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(x, y_relu, color='green')\n",
    "plt.title(\"ReLU Activation Function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(x, y_tanh, color='red')\n",
    "plt.title(\"Tanh Activation Function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5ef37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8: Use Keras to build and train a simple multilayer neural network on the MNIST digits dataset. Print the training accuracy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the input data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=128, verbose=1)\n",
    "\n",
    "# Print training accuracy\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store training history\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# If training and validation accuracy both increase and stabilize, it indicates good learning.\n",
    "# If training accuracy continues to improve but validation accuracy plateaus or drops, it suggests overfitting.\n",
    "# If both accuracies are low, the model might be underfitting or need more epochs or better architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6490094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Question 10: You are working on a project for a bank that wants to automatically detect\n",
    "fraudulent transactions. The dataset is large, imbalanced, and contains structured\n",
    "features like transaction amount, merchant ID, and customer location. The goal is to\n",
    "classify each transaction as fraudulent or legitimate.\n",
    "Explain your real-time data science workflow:\n",
    "● How would you design a deep learning model (perceptron or multilayer NN)?\n",
    "● Which activation function and loss function would you use, and why?\n",
    "● How would you train and evaluate the model, considering class imbalance?\n",
    "● Which optimizer would be suitable, and how would you prevent overfitting?'''\n",
    "\n",
    "'''Step 1: Problem Understanding\n",
    "- Objective: Automatically detect fraudulent transactions from large, imbalanced data.\n",
    "- Data: Structured tabular features (transaction amount, merchant ID, customer location, etc.)\n",
    "- Challenge: Class imbalance (~1% fraud), requiring careful model design and training strategy.\n",
    "\n",
    "Step 2: Model Design\n",
    "- Architecture: Multilayer neural network (deep perceptron).\n",
    "- Input features: \n",
    "  - Numeric features → standardized using StandardScaler.\n",
    "  - Categorical features → converted using label encoding and embedded into low-dimensional vectors.\n",
    "- Hidden Layers: \n",
    "  - Dense layers with ReLU activation and dropout to reduce overfitting.\n",
    "  - Batch normalization layers for faster convergence and stable training.\n",
    "- Output Layer: \n",
    "  - Single neuron with sigmoid activation for binary classification (fraudulent or legitimate).\n",
    "\n",
    "Step 3: Activation and Loss Function\n",
    "- Activation Function: ReLU in hidden layers to introduce non-linearity and prevent vanishing gradients.\n",
    "- Output Activation: Sigmoid (since the output is binary: 0 = legitimate, 1 = fraud).\n",
    "- Loss Function: Binary Crossentropy (suitable for binary classification problems).\n",
    "  - Optionally, Focal Loss can be used to handle extreme class imbalance by giving higher weight to rare fraud cases.\n",
    "\n",
    "Step 4: Training and Evaluation\n",
    "- Handle class imbalance using 'class_weight' computed from training labels.\n",
    "- Split the data into training, validation, and test sets (stratified split to maintain class ratio).\n",
    "- Use callbacks such as:\n",
    "  - EarlyStopping: stops training when validation loss stops improving.\n",
    "  - ReduceLROnPlateau: dynamically reduces learning rate on performance plateau.\n",
    "  - ModelCheckpoint: saves the best model during training.\n",
    "- Evaluate model performance using:\n",
    "  - Precision, Recall, and F1-score (important for imbalanced data).\n",
    "  - ROC AUC and Average Precision (AUC-PR) for overall quality.\n",
    "\n",
    "Step 5: Optimizer\n",
    "- Adam optimizer used for faster convergence and adaptive learning rate.\n",
    "- Alternative: RMSprop or plain Gradient Descent could also be tested.\n",
    "- Regularization techniques:\n",
    "  - Dropout layers.\n",
    "  - L2 regularization.\n",
    "  - Early stopping.\n",
    "\n",
    "Step 6: Interpretation of Results\n",
    "- Training vs validation curves show overfitting or underfitting behavior.\n",
    "- Precision-Recall trade-off helps select an optimal classification threshold (maximizing F1-score).\n",
    "- Final evaluation includes AUC-ROC and AUC-PR metrics to measure overall discrimination power.\n",
    "\n",
    "Step 7: Deployment Preparation\n",
    "- Save model, encoders, and scaler for consistent preprocessing in production.\n",
    "- Deployed model can perform real-time predictions on incoming transactions.\n",
    "\n",
    "Step 8: Summary\n",
    "- Deep learning can effectively capture complex non-linear relationships in structured data.\n",
    "- For fraud detection, handling imbalance, preventing overfitting, and proper evaluation are critical.\n",
    "- This workflow provides a practical, end-to-end approach to developing a robust fraud detection system.\n",
    "''''\n",
    "\n",
    "# Practical, end-to-end Keras pipeline for a fraud-detection model on tabular data.\n",
    "# Assumes a pandas DataFrame `df` with a binary target column named 'label'\n",
    "# and some example features: 'amount', 'merchant_id', 'customer_id', 'location', 'device', 'hour'.\n",
    "# Designed for Jupyter — run cells sequentially.\n",
    "\n",
    "# Requirements: numpy, pandas, scikit-learn, matplotlib, tensorflow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except Exception:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install --upgrade pip\n",
    "    !{sys.executable} -m pip install tensorflow\n",
    "    import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load data (replace with your CSV / DB load)\n",
    "# -----------------------------\n",
    "# Example placeholder: replace 'transactions.csv' with your dataset path.\n",
    "# df = pd.read_csv('transactions.csv')\n",
    "# For demonstration, build a small synthetic dataset structure (not for production use).\n",
    "num_samples = 20000\n",
    "rng = np.random.default_rng(42)\n",
    "df = pd.DataFrame({\n",
    "    'amount': np.abs(rng.normal(loc=50, scale=40, size=num_samples)),            # numeric\n",
    "    'merchant_id': rng.integers(0, 2000, size=num_samples).astype(str),         # high-cardinality categorical\n",
    "    'customer_id': rng.integers(0, 5000, size=num_samples).astype(str),\n",
    "    'location': rng.choice(['US', 'IN', 'GB', 'DE', 'FR'], size=num_samples),\n",
    "    'device': rng.choice(['mobile', 'web', 'pos'], size=num_samples),\n",
    "    'hour': rng.integers(0, 24, size=num_samples),\n",
    "    # Simulated imbalance: ~1% fraud\n",
    "    'label': (rng.random(num_samples) < 0.01).astype(int)\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Preprocessing\n",
    "# -----------------------------\n",
    "# Define categorical and numeric columns\n",
    "cat_cols = ['merchant_id', 'customer_id', 'location', 'device']\n",
    "num_cols = ['amount', 'hour']\n",
    "target_col = 'label'\n",
    "\n",
    "# Simple label encoding for categories and keep mapping sizes for embeddings\n",
    "encoders = {}\n",
    "vocab_sizes = {}\n",
    "for c in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[c] = le.fit_transform(df[c].astype(str))\n",
    "    encoders[c] = le\n",
    "    vocab_sizes[c] = int(df[c].nunique()) + 1\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# Time-aware split suggestion: if you have a timestamp, split by time.\n",
    "# Here we do stratified random split to preserve class distribution for demo\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[target_col], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.125, stratify=train_df[target_col], random_state=42)  # 0.125 of 0.8 -> 0.10\n",
    "\n",
    "# Build dataset inputs for Keras\n",
    "def df_to_keras_inputs(df_):\n",
    "    X_num = df_[num_cols].values.astype(np.float32)\n",
    "    X_cat = {c: df_[c].values.astype(np.int32) for c in cat_cols}\n",
    "    y = df_[target_col].values.astype(np.int32)\n",
    "    inputs = {**{f'num_{i}': X_num[:, i:i+1] for i in range(X_num.shape[1])}, **X_cat}\n",
    "    # Another convenient form: provide numeric vector separately\n",
    "    return X_num, X_cat, y\n",
    "\n",
    "X_num_train, X_cat_train, y_train = df_to_keras_inputs(train_df)\n",
    "X_num_val,   X_cat_val,   y_val   = df_to_keras_inputs(val_df)\n",
    "X_num_test,  X_cat_test,  y_test  = df_to_keras_inputs(test_df)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Model: embeddings for high-cardinality categories + numeric tower\n",
    "# -----------------------------\n",
    "from tensorflow.keras import layers, regularizers, Model, Input\n",
    "\n",
    "# Numeric input\n",
    "numeric_input = Input(shape=(len(num_cols),), name='numeric_input')\n",
    "\n",
    "# Categorical embedding inputs\n",
    "cat_inputs = {}\n",
    "embeddings = []\n",
    "for c in cat_cols:\n",
    "    inp = Input(shape=(1,), dtype='int32', name=c)\n",
    "    cat_inputs[c] = inp\n",
    "    vocab = vocab_sizes[c]\n",
    "    # embedding dimension heuristic\n",
    "    emb_dim = min(50, (vocab // 2) + 1)\n",
    "    emb = layers.Embedding(input_dim=vocab, output_dim=emb_dim, embeddings_regularizer=regularizers.l2(1e-6))(inp)\n",
    "    emb = layers.Reshape((emb_dim,))(emb)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# Concatenate everything\n",
    "x = layers.Concatenate()( [numeric_input] + embeddings )\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "output = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "model_inputs = [numeric_input] + list(cat_inputs.values())\n",
    "model = Model(inputs=model_inputs, outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Prepare Keras inputs from arrays/dicts\n",
    "# -----------------------------\n",
    "def make_input_dict(X_num, X_cat):\n",
    "    d = {'numeric_input': X_num}\n",
    "    d.update(X_cat)\n",
    "    return d\n",
    "\n",
    "train_inputs = make_input_dict(X_num_train, X_cat_train)\n",
    "val_inputs   = make_input_dict(X_num_val, X_cat_val)\n",
    "test_inputs  = make_input_dict(X_num_test, X_cat_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Handle class imbalance: compute class weights and optional focal loss\n",
    "# -----------------------------\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = {int(c): w for c, w in zip(classes, class_weights)}\n",
    "\n",
    "# Optional: Focal loss implementation\n",
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = (y_true * y_pred) + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "        return tf.reduce_mean(alpha_factor * modulating_factor * bce)\n",
    "    return loss\n",
    "\n",
    "# Choose loss: either 'binary_crossentropy' or focal_loss()\n",
    "use_focal = False\n",
    "if use_focal:\n",
    "    loss_fn = focal_loss(gamma=2.0, alpha=0.25)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=loss_fn, metrics=['accuracy'])\n",
    "else:\n",
    "    loss_fn = 'binary_crossentropy'\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Callbacks\n",
    "# -----------------------------\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6),\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_fraud_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Training\n",
    "# -----------------------------\n",
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    y_train,\n",
    "    validation_data=(val_inputs, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=1024,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Print final training accuracy\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "print(f\"Final training accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Evaluation on test set\n",
    "# -----------------------------\n",
    "y_pred_proba = model.predict(test_inputs, batch_size=1024).ravel()\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)  # default threshold 0.5\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', zero_division=0)\n",
    "ap = average_precision_score(y_test, y_pred_proba)\n",
    "roc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "print(f\"Average Precision (AUC-PR): {ap:.4f}, ROC AUC: {roc:.4f}\")\n",
    "\n",
    "# Threshold tuning (choose threshold to maximize F1 on validation set)\n",
    "y_val_proba = model.predict(val_inputs, batch_size=1024).ravel()\n",
    "best_thresh = 0.5\n",
    "best_f1 = 0.0\n",
    "for t in np.linspace(0.01, 0.99, 99):\n",
    "    p = (y_val_proba >= t).astype(int)\n",
    "    pr, rc, f1s, _ = precision_recall_fscore_support(y_val, p, average='binary', zero_division=0)\n",
    "    if f1s > best_f1:\n",
    "        best_f1 = f1s\n",
    "        best_thresh = t\n",
    "print(f\"Best threshold on validation for F1: {best_thresh:.3f} (F1={best_f1:.4f})\")\n",
    "\n",
    "# Apply best threshold to test\n",
    "y_pred_best = (y_pred_proba >= best_thresh).astype(int)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_best, average='binary', zero_division=0)\n",
    "print(f\"Test (best-threshold) Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9) Plot training curves\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 10) Save artifacts: model, scaler, encoders\n",
    "# -----------------------------\n",
    "model.save('final_fraud_model')\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "joblib.dump(encoders, 'encoders.joblib')\n",
    "\n",
    "# End of pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
